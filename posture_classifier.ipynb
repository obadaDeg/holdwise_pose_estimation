{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this Application write this in your terminal:\n",
    "\n",
    "```py -3.10 -m venv myenv```\n",
    "\n",
    "```.\\myenv\\Scripts\\Activate```\n",
    "\n",
    "select the kernal to be myenv (Python 3.10.0)\n",
    "\n",
    "```py --version``` insure this version is 3.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n"
     ]
    }
   ],
   "source": [
    "!py --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **this cell could take some time to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow is already installed.\n",
      "tensorflow-hub not found. Installing...\n",
      "tensorflow-hub installed successfully.\n",
      "numpy is already installed.\n",
      "matplotlib is already installed.\n",
      "pandas is already installed.\n",
      "cv2 is already installed.\n",
      "keras is already installed.\n",
      "pillow not found. Installing...\n",
      "pillow installed successfully.\n",
      "WARNING:tensorflow:From c:\\Users\\obada\\Desktop\\HoldWise Dataset\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import importlib\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "\n",
    "# Define the libraries and their pip install commands\n",
    "libraries = {\n",
    "    \"tensorflow\": \"pip install tensorflow\",\n",
    "    \"tensorflow-hub\": \"pip install tensorflow-hub\",\n",
    "    \"numpy\": \"pip install numpy\",\n",
    "    \"matplotlib\": \"pip install matplotlib\",\n",
    "    \"pandas\": \"pip install pandas\",\n",
    "    \"cv2\": \"pip install opencv-python\", \n",
    "    \"keras\": \"pip install keras\",\n",
    "    \"pillow\": \"pip install pillow\",\n",
    "}\n",
    "\n",
    "for library, command in libraries.items():\n",
    "    try:\n",
    "        importlib.import_module(library)\n",
    "        print(f\"{library} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{library} not found. Installing...\")\n",
    "        try:\n",
    "            subprocess.run(command.split(), check=True)\n",
    "            print(f\"{library} installed successfully.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {library}. Error: {e}\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import models, layers, applications, utils, preprocessing, callbacks, optimizers\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(True) # Log device placement (on which device the operation is executed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_path, output_folder, frame_rate=1):\n",
    "    \"\"\"\n",
    "    Extracts frames from a video at a specified frame rate.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        output_folder (str): Folder to save the extracted frames.\n",
    "        frame_rate (int): Number of frames to save per second of video.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if frame_rate <= 0:\n",
    "        logging.error(\"Error: Frame rate must be a positive integer.\")\n",
    "        return\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        logging.error(f\"Error: Could not open video file '{video_path}'.\")\n",
    "        return\n",
    "\n",
    "    # Get the video's frame rate and total number of frames\n",
    "    video_fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    if video_fps <= 0:\n",
    "        logging.error(\"Error: Invalid video FPS. Please check the video file.\")\n",
    "        video.release()\n",
    "        return\n",
    "\n",
    "    if frame_rate > video_fps:\n",
    "        logging.warning(f\"Warning: Frame rate ({frame_rate}) exceeds video FPS ({video_fps}). Adjusting to match video FPS.\")\n",
    "        frame_rate = video_fps\n",
    "\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    logging.info(f\"Video FPS: {video_fps}, Total Frames: {frame_count}\")\n",
    "\n",
    "    # Calculate the frame interval for the desired frame rate\n",
    "    frame_interval = video_fps // frame_rate\n",
    "\n",
    "    # Extract and save frames\n",
    "    frame_idx = 0\n",
    "    saved_frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Save the frame at the specified interval\n",
    "        if frame_idx % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{saved_frame_idx:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_frame_idx += 1\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "\n",
    "    if saved_frame_idx == 0:\n",
    "        logging.warning(\"No frames were saved. Adjust the frame rate and try again.\")\n",
    "    else:\n",
    "        logging.info(f\"{saved_frame_idx} frames saved to '{output_folder}'.\")\n",
    "        \n",
    "    return saved_frame_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_extraction_folders = [\n",
    "    \"workspace/good_pose\",\n",
    "    \"workspace/bad_pose\",\n",
    "    \"workspace/cant_determine\"\n",
    "]\n",
    "\n",
    "\n",
    "def process_session(session_path, label, output_dir, frame_rate=1):\n",
    "    \"\"\"\n",
    "    Processes a single session folder, extracting frames and mapping to sensor data.\n",
    "    \"\"\"\n",
    "    video_path, json_path = None, None\n",
    "\n",
    "    for file_name in os.listdir(session_path):\n",
    "        file_path = os.path.join(session_path, file_name)\n",
    "        lower = file_name.lower()\n",
    "        if lower.endswith('.mp4'):\n",
    "            video_path = file_path\n",
    "        elif lower.endswith('.json'):\n",
    "            json_path = file_path\n",
    "\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        logging.warning(f\"No valid video found in: {session_path}\")\n",
    "        return\n",
    "    if not json_path or not os.path.exists(json_path):\n",
    "        logging.warning(f\"No valid JSON file found in: {session_path}\")\n",
    "        return\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        sensor_data = json.load(f)\n",
    "\n",
    "    gyro_list = sensor_data.get(\"gyroscopeData\", [])\n",
    "    accel_list = sensor_data.get(\"accelerometerData\", [])\n",
    "\n",
    "    session_name = Path(session_path).name\n",
    "    session_output = os.path.join(output_dir, label, session_name)\n",
    "    os.makedirs(session_output, exist_ok=True)\n",
    "\n",
    "    saved_frames = video_to_frames(video_path, session_output, frame_rate)\n",
    "    if saved_frames == 0:\n",
    "        logging.warning(\"No frames were extracted; skipping sensor mapping.\")\n",
    "        return\n",
    "\n",
    "    if not gyro_list or not accel_list:\n",
    "        logging.warning(\"Missing sensor data in JSON; skipping sensor mapping.\")\n",
    "        return\n",
    "\n",
    "    frame_files = sorted([f for f in os.listdir(session_output) if f.lower().endswith('.jpg')])\n",
    "    csv_path = os.path.join(session_output, \"frame_sensor_mapping.csv\")\n",
    "\n",
    "    with open(csv_path, 'w') as csv_file:\n",
    "        csv_file.write(\"frame_file,frame_idx,sensor_idx,gyro_x,gyro_y,gyro_z,accel_x,accel_y,accel_z\\n\")\n",
    "        M = saved_frames\n",
    "        N = len(gyro_list)\n",
    "        L = len(accel_list)\n",
    "\n",
    "        for i, frame_file in enumerate(frame_files):\n",
    "            gyro_idx = int((i / M) * N)\n",
    "            accel_idx = int((i / M) * L)\n",
    "\n",
    "            gyro_data = gyro_list[gyro_idx]\n",
    "            accel_data = accel_list[accel_idx]\n",
    "\n",
    "            csv_file.write(\n",
    "                f\"{frame_file},{i},{gyro_idx},\"\n",
    "                f\"{gyro_data.get('x', 0)},{gyro_data.get('y', 0)},{gyro_data.get('z', 0)},\"\n",
    "                f\"{accel_data.get('x', 0)},{accel_data.get('y', 0)},{accel_data.get('z', 0)}\\n\"\n",
    "            )\n",
    "\n",
    "            # old_path = os.path.join(session_output, frame_file)\n",
    "            # base, ext = os.path.splitext(frame_file)\n",
    "            # new_filename = f\"{base}_sensor_{gyro_idx}{ext}\"\n",
    "            # new_path = os.path.join(session_output, new_filename)\n",
    "            # os.rename(old_path, new_path)\n",
    "\n",
    "    logging.info(f\"Frame-to-sensor mapping complete for: {session_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 14:17:22,744 - INFO - Video FPS: 29, Total Frames: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 14:17:22,803 - INFO - Video FPS: 30, Total Frames: 525\n",
      "2025-01-14 14:17:22,811 - INFO - Video FPS: 29, Total Frames: 1217\n",
      "2025-01-14 14:17:28,676 - INFO - 175 frames saved to 'extracted_frames\\workspace/bad_pose\\10'.\n",
      "2025-01-14 14:17:28,677 - WARNING - Missing sensor data in JSON; skipping sensor mapping.\n",
      "2025-01-14 14:17:28,689 - INFO - Video FPS: 29, Total Frames: 311\n",
      "2025-01-14 14:17:32,889 - INFO - 156 frames saved to 'extracted_frames\\workspace/bad_pose\\12'.\n",
      "2025-01-14 14:17:32,898 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\12\n",
      "2025-01-14 14:17:32,910 - INFO - Video FPS: 29, Total Frames: 1018\n",
      "2025-01-14 14:17:33,309 - INFO - 376 frames saved to 'extracted_frames\\workspace/good_pose\\1'.\n",
      "2025-01-14 14:17:33,314 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\1\n",
      "2025-01-14 14:17:33,335 - INFO - Video FPS: 29, Total Frames: 667\n",
      "2025-01-14 14:17:39,535 - INFO - 609 frames saved to 'extracted_frames\\workspace/cant_determine\\14'.\n",
      "2025-01-14 14:17:39,542 - INFO - Frame-to-sensor mapping complete for: workspace\\cant_determine\\14\n",
      "2025-01-14 14:17:39,555 - INFO - Video FPS: 29, Total Frames: 391\n",
      "2025-01-14 14:17:43,401 - INFO - 334 frames saved to 'extracted_frames\\workspace/good_pose\\11'.\n",
      "2025-01-14 14:17:43,402 - WARNING - Missing sensor data in JSON; skipping sensor mapping.\n",
      "2025-01-14 14:17:43,411 - INFO - Video FPS: 29, Total Frames: 1246\n",
      "2025-01-14 14:17:44,819 - INFO - 196 frames saved to 'extracted_frames\\workspace/cant_determine\\24'.\n",
      "2025-01-14 14:17:44,824 - INFO - Frame-to-sensor mapping complete for: workspace\\cant_determine\\24\n",
      "2025-01-14 14:17:44,842 - INFO - Video FPS: 29, Total Frames: 1770\n",
      "2025-01-14 14:17:46,809 - INFO - 509 frames saved to 'extracted_frames\\workspace/bad_pose\\13'.\n",
      "2025-01-14 14:17:46,816 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\13\n",
      "2025-01-14 14:17:46,825 - INFO - Video FPS: 29, Total Frames: 442\n",
      "2025-01-14 14:17:52,536 - INFO - 221 frames saved to 'extracted_frames\\workspace/bad_pose\\16'.\n",
      "2025-01-14 14:17:52,540 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\16\n",
      "2025-01-14 14:17:52,551 - INFO - Video FPS: 29, Total Frames: 478\n",
      "2025-01-14 14:17:58,771 - INFO - 239 frames saved to 'extracted_frames\\workspace/bad_pose\\18'.\n",
      "2025-01-14 14:17:58,777 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\18\n",
      "2025-01-14 14:17:58,789 - INFO - Video FPS: 29, Total Frames: 1016\n",
      "2025-01-14 14:18:00,029 - INFO - 623 frames saved to 'extracted_frames\\workspace/good_pose\\15'.\n",
      "2025-01-14 14:18:00,036 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\15\n",
      "2025-01-14 14:18:00,048 - INFO - Video FPS: 29, Total Frames: 826\n",
      "2025-01-14 14:18:08,098 - INFO - 885 frames saved to 'extracted_frames\\workspace/cant_determine\\51'.\n",
      "2025-01-14 14:18:08,107 - INFO - Frame-to-sensor mapping complete for: workspace\\cant_determine\\51\n",
      "2025-01-14 14:18:08,109 - WARNING - No valid video found in: workspace\\cant_determine\\missing data\n",
      "2025-01-14 14:18:10,605 - INFO - 413 frames saved to 'extracted_frames\\workspace/good_pose\\17'.\n",
      "2025-01-14 14:18:10,610 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\17\n",
      "2025-01-14 14:18:10,619 - INFO - Video FPS: 29, Total Frames: 549\n",
      "2025-01-14 14:18:11,786 - INFO - 508 frames saved to 'extracted_frames\\workspace/bad_pose\\19'.\n",
      "2025-01-14 14:18:11,793 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\19\n",
      "2025-01-14 14:18:11,808 - INFO - Video FPS: 29, Total Frames: 1697\n",
      "2025-01-14 14:18:17,181 - INFO - 275 frames saved to 'extracted_frames\\workspace/good_pose\\2'.\n",
      "2025-01-14 14:18:17,186 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\2\n",
      "2025-01-14 14:18:17,197 - INFO - Video FPS: 29, Total Frames: 946\n",
      "2025-01-14 14:18:28,078 - INFO - 473 frames saved to 'extracted_frames\\workspace/good_pose\\21'.\n",
      "2025-01-14 14:18:28,086 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\21\n",
      "2025-01-14 14:18:28,096 - INFO - Video FPS: 30, Total Frames: 56\n",
      "2025-01-14 14:18:28,704 - INFO - 19 frames saved to 'extracted_frames\\workspace/good_pose\\23'.\n",
      "2025-01-14 14:18:28,706 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\23\n",
      "2025-01-14 14:18:28,720 - INFO - Video FPS: 29, Total Frames: 358\n",
      "2025-01-14 14:18:31,514 - INFO - 849 frames saved to 'extracted_frames\\workspace/bad_pose\\20'.\n",
      "2025-01-14 14:18:31,524 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\20\n",
      "2025-01-14 14:18:31,537 - INFO - Video FPS: 29, Total Frames: 319\n",
      "2025-01-14 14:18:32,812 - INFO - 179 frames saved to 'extracted_frames\\workspace/good_pose\\25'.\n",
      "2025-01-14 14:18:32,815 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\25\n",
      "2025-01-14 14:18:32,830 - INFO - Video FPS: 29, Total Frames: 7672\n",
      "2025-01-14 14:18:35,389 - INFO - 160 frames saved to 'extracted_frames\\workspace/bad_pose\\26'.\n",
      "2025-01-14 14:18:35,396 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\26\n",
      "2025-01-14 14:18:35,429 - INFO - Video FPS: 26, Total Frames: 53\n",
      "2025-01-14 14:18:36,004 - INFO - 27 frames saved to 'extracted_frames\\workspace/bad_pose\\29'.\n",
      "2025-01-14 14:18:36,006 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\29\n",
      "2025-01-14 14:18:36,016 - INFO - Video FPS: 29, Total Frames: 261\n",
      "2025-01-14 14:18:39,020 - INFO - 131 frames saved to 'extracted_frames\\workspace/bad_pose\\3'.\n",
      "2025-01-14 14:18:39,025 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\3\n",
      "2025-01-14 14:18:39,058 - INFO - Video FPS: 26, Total Frames: 53\n",
      "2025-01-14 14:18:39,647 - INFO - 27 frames saved to 'extracted_frames\\workspace/bad_pose\\38'.\n",
      "2025-01-14 14:18:39,649 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\38\n",
      "2025-01-14 14:18:39,677 - INFO - Video FPS: 29, Total Frames: 7473\n",
      "2025-01-14 14:20:03,873 - INFO - 3836 frames saved to 'extracted_frames\\workspace/good_pose\\30'.\n",
      "2025-01-14 14:20:03,908 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\30\n",
      "2025-01-14 14:20:03,919 - INFO - Video FPS: 29, Total Frames: 146\n",
      "2025-01-14 14:20:05,660 - INFO - 73 frames saved to 'extracted_frames\\workspace/good_pose\\32'.\n",
      "2025-01-14 14:20:05,662 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\32\n",
      "2025-01-14 14:20:05,674 - INFO - Video FPS: 29, Total Frames: 355\n",
      "2025-01-14 14:20:08,542 - INFO - 3737 frames saved to 'extracted_frames\\workspace/bad_pose\\39'.\n",
      "2025-01-14 14:20:08,574 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\39\n",
      "2025-01-14 14:20:08,600 - INFO - Video FPS: 29, Total Frames: 5596\n",
      "2025-01-14 14:20:09,914 - INFO - 178 frames saved to 'extracted_frames\\workspace/good_pose\\33'.\n",
      "2025-01-14 14:20:09,917 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\33\n",
      "2025-01-14 14:20:09,935 - INFO - Video FPS: 29, Total Frames: 698\n",
      "2025-01-14 14:20:18,278 - INFO - 349 frames saved to 'extracted_frames\\workspace/good_pose\\34'.\n",
      "2025-01-14 14:20:18,283 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\34\n",
      "2025-01-14 14:20:18,304 - INFO - Video FPS: 29, Total Frames: 7672\n",
      "2025-01-14 14:21:12,944 - INFO - 2798 frames saved to 'extracted_frames\\workspace/bad_pose\\42'.\n",
      "2025-01-14 14:21:12,970 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\42\n",
      "2025-01-14 14:21:12,989 - INFO - Video FPS: 29, Total Frames: 2839\n",
      "2025-01-14 14:21:45,492 - INFO - 1420 frames saved to 'extracted_frames\\workspace/bad_pose\\44'.\n",
      "2025-01-14 14:21:45,512 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\44\n",
      "2025-01-14 14:21:45,524 - INFO - Video FPS: 29, Total Frames: 154\n",
      "2025-01-14 14:21:47,367 - INFO - 77 frames saved to 'extracted_frames\\workspace/bad_pose\\45'.\n",
      "2025-01-14 14:21:47,370 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\45\n",
      "2025-01-14 14:21:47,382 - INFO - Video FPS: 29, Total Frames: 711\n",
      "2025-01-14 14:21:49,499 - INFO - 3836 frames saved to 'extracted_frames\\workspace/good_pose\\37'.\n",
      "2025-01-14 14:21:49,535 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\37\n",
      "2025-01-14 14:21:49,546 - INFO - Video FPS: 29, Total Frames: 94\n",
      "2025-01-14 14:21:50,671 - INFO - 47 frames saved to 'extracted_frames\\workspace/good_pose\\4'.\n",
      "2025-01-14 14:21:50,673 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\4\n",
      "2025-01-14 14:21:50,724 - INFO - Video FPS: 29, Total Frames: 4684\n",
      "2025-01-14 14:21:56,038 - INFO - 356 frames saved to 'extracted_frames\\workspace/bad_pose\\46'.\n",
      "2025-01-14 14:21:56,051 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\46\n",
      "2025-01-14 14:21:56,089 - INFO - Video FPS: 29, Total Frames: 64\n",
      "2025-01-14 14:21:56,933 - INFO - 32 frames saved to 'extracted_frames\\workspace/bad_pose\\47'.\n",
      "2025-01-14 14:21:56,935 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\47\n",
      "2025-01-14 14:21:56,947 - INFO - Video FPS: 29, Total Frames: 316\n",
      "2025-01-14 14:22:00,808 - INFO - 158 frames saved to 'extracted_frames\\workspace/bad_pose\\49'.\n",
      "2025-01-14 14:22:00,814 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\49\n",
      "2025-01-14 14:22:00,826 - INFO - Video FPS: 29, Total Frames: 529\n",
      "2025-01-14 14:22:07,008 - INFO - 265 frames saved to 'extracted_frames\\workspace/bad_pose\\50'.\n",
      "2025-01-14 14:22:07,012 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\50\n",
      "2025-01-14 14:22:07,024 - INFO - Video FPS: 27, Total Frames: 70\n",
      "2025-01-14 14:22:07,824 - INFO - 35 frames saved to 'extracted_frames\\workspace/bad_pose\\52'.\n",
      "2025-01-14 14:22:07,826 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\52\n",
      "2025-01-14 14:22:07,838 - INFO - Video FPS: 30, Total Frames: 3169\n",
      "2025-01-14 14:22:38,994 - INFO - 1057 frames saved to 'extracted_frames\\workspace/bad_pose\\54'.\n",
      "2025-01-14 14:22:39,005 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\54\n",
      "2025-01-14 14:22:39,020 - INFO - Video FPS: 29, Total Frames: 5171\n",
      "2025-01-14 14:22:47,851 - INFO - 2342 frames saved to 'extracted_frames\\workspace/good_pose\\40'.\n",
      "2025-01-14 14:22:47,873 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\40\n",
      "2025-01-14 14:22:47,899 - INFO - Video FPS: 29, Total Frames: 108\n",
      "2025-01-14 14:22:49,185 - INFO - 54 frames saved to 'extracted_frames\\workspace/good_pose\\41'.\n",
      "2025-01-14 14:22:49,187 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\41\n",
      "2025-01-14 14:22:49,197 - INFO - Video FPS: 29, Total Frames: 459\n",
      "2025-01-14 14:22:54,649 - INFO - 230 frames saved to 'extracted_frames\\workspace/good_pose\\48'.\n",
      "2025-01-14 14:22:54,653 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\48\n",
      "2025-01-14 14:22:54,664 - INFO - Video FPS: 30, Total Frames: 2567\n",
      "2025-01-14 14:23:18,660 - INFO - 856 frames saved to 'extracted_frames\\workspace/good_pose\\53'.\n",
      "2025-01-14 14:23:18,668 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\53\n",
      "2025-01-14 14:23:18,684 - INFO - Video FPS: 29, Total Frames: 1976\n",
      "2025-01-14 14:23:38,186 - INFO - 2586 frames saved to 'extracted_frames\\workspace/bad_pose\\55'.\n",
      "2025-01-14 14:23:38,207 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\55\n",
      "2025-01-14 14:23:38,223 - INFO - Video FPS: 29, Total Frames: 1867\n",
      "2025-01-14 14:23:44,150 - INFO - 988 frames saved to 'extracted_frames\\workspace/good_pose\\57'.\n",
      "2025-01-14 14:23:44,160 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\57\n",
      "2025-01-14 14:23:44,170 - INFO - Video FPS: 30, Total Frames: 23\n",
      "2025-01-14 14:23:44,399 - INFO - 8 frames saved to 'extracted_frames\\workspace/good_pose\\58'.\n",
      "2025-01-14 14:23:44,401 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\58\n",
      "2025-01-14 14:23:44,413 - INFO - Video FPS: 30, Total Frames: 2015\n",
      "2025-01-14 14:24:01,040 - INFO - 934 frames saved to 'extracted_frames\\workspace/bad_pose\\56'.\n",
      "2025-01-14 14:24:01,048 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\56\n",
      "2025-01-14 14:24:01,064 - INFO - Video FPS: 29, Total Frames: 1859\n",
      "2025-01-14 14:24:04,023 - INFO - 672 frames saved to 'extracted_frames\\workspace/good_pose\\62'.\n",
      "2025-01-14 14:24:04,030 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\62\n",
      "2025-01-14 14:24:04,053 - INFO - Video FPS: 29, Total Frames: 1062\n",
      "2025-01-14 14:24:17,513 - INFO - 531 frames saved to 'extracted_frames\\workspace/good_pose\\63'.\n",
      "2025-01-14 14:24:17,520 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\63\n",
      "2025-01-14 14:24:17,538 - INFO - Video FPS: 29, Total Frames: 2911\n",
      "2025-01-14 14:24:24,446 - INFO - 930 frames saved to 'extracted_frames\\workspace/bad_pose\\59'.\n",
      "2025-01-14 14:24:24,459 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\59\n",
      "2025-01-14 14:24:24,472 - INFO - Video FPS: 29, Total Frames: 1930\n",
      "2025-01-14 14:24:47,765 - INFO - 965 frames saved to 'extracted_frames\\workspace/bad_pose\\61'.\n",
      "2025-01-14 14:24:47,776 - INFO - Frame-to-sensor mapping complete for: workspace\\bad_pose\\61\n",
      "2025-01-14 14:24:47,798 - INFO - Video FPS: 30, Total Frames: 279\n",
      "2025-01-14 14:24:50,601 - INFO - 93 frames saved to 'extracted_frames\\workspace/bad_pose\\9'.\n",
      "2025-01-14 14:24:50,602 - WARNING - Missing sensor data in JSON; skipping sensor mapping.\n",
      "2025-01-14 14:24:51,966 - INFO - 1456 frames saved to 'extracted_frames\\workspace/good_pose\\64'.\n",
      "2025-01-14 14:24:51,980 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\64\n",
      "2025-01-14 14:24:52,035 - INFO - Video FPS: 29, Total Frames: 5643\n",
      "2025-01-14 14:25:52,132 - INFO - 2822 frames saved to 'extracted_frames\\workspace/good_pose\\65'.\n",
      "2025-01-14 14:25:52,153 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\65\n",
      "2025-01-14 14:25:52,181 - INFO - Video FPS: 29, Total Frames: 5864\n",
      "2025-01-14 14:26:52,501 - INFO - 2932 frames saved to 'extracted_frames\\workspace/good_pose\\66'.\n",
      "2025-01-14 14:26:52,526 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\66\n",
      "2025-01-14 14:26:52,545 - INFO - Video FPS: 29, Total Frames: 12475\n",
      "2025-01-14 14:28:59,909 - INFO - 6238 frames saved to 'extracted_frames\\workspace/good_pose\\67'.\n",
      "2025-01-14 14:28:59,953 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\67\n",
      "2025-01-14 14:28:59,962 - INFO - Video FPS: 29, Total Frames: 160\n",
      "2025-01-14 14:29:01,598 - INFO - 80 frames saved to 'extracted_frames\\workspace/good_pose\\68'.\n",
      "2025-01-14 14:29:01,601 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\68\n",
      "2025-01-14 14:29:01,610 - INFO - Video FPS: 29, Total Frames: 1793\n",
      "2025-01-14 14:29:20,047 - INFO - 897 frames saved to 'extracted_frames\\workspace/good_pose\\69'.\n",
      "2025-01-14 14:29:20,055 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\69\n",
      "2025-01-14 14:29:20,072 - INFO - Video FPS: 29, Total Frames: 633\n",
      "2025-01-14 14:29:26,623 - INFO - 317 frames saved to 'extracted_frames\\workspace/good_pose\\7'.\n",
      "2025-01-14 14:29:26,627 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\7\n",
      "2025-01-14 14:29:26,638 - INFO - Video FPS: 29, Total Frames: 2239\n",
      "2025-01-14 14:29:49,278 - INFO - 1120 frames saved to 'extracted_frames\\workspace/good_pose\\70'.\n",
      "2025-01-14 14:29:49,287 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\70\n",
      "2025-01-14 14:29:49,304 - INFO - Video FPS: 30, Total Frames: 2430\n",
      "2025-01-14 14:30:09,373 - INFO - 810 frames saved to 'extracted_frames\\workspace/good_pose\\71'.\n",
      "2025-01-14 14:30:09,380 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\71\n",
      "2025-01-14 14:30:09,395 - INFO - Video FPS: 29, Total Frames: 2066\n",
      "2025-01-14 14:30:33,046 - INFO - 1033 frames saved to 'extracted_frames\\workspace/good_pose\\72'.\n",
      "2025-01-14 14:30:33,055 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\72\n",
      "2025-01-14 14:30:33,071 - INFO - Video FPS: 30, Total Frames: 2196\n",
      "2025-01-14 14:30:51,803 - INFO - 732 frames saved to 'extracted_frames\\workspace/good_pose\\73'.\n",
      "2025-01-14 14:30:51,809 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\73\n",
      "2025-01-14 14:30:51,827 - INFO - Video FPS: 30, Total Frames: 3111\n",
      "2025-01-14 14:31:18,164 - INFO - 1037 frames saved to 'extracted_frames\\workspace/good_pose\\74'.\n",
      "2025-01-14 14:31:18,172 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\74\n",
      "2025-01-14 14:31:18,187 - INFO - Video FPS: 30, Total Frames: 1933\n",
      "2025-01-14 14:31:35,000 - INFO - 645 frames saved to 'extracted_frames\\workspace/good_pose\\75'.\n",
      "2025-01-14 14:31:35,008 - INFO - Frame-to-sensor mapping complete for: workspace\\good_pose\\75\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"extracted_frames\"\n",
    "# frame_rate = 30\n",
    "frame_rate = 10 # for demonstration purposes\n",
    "\n",
    "def process_label(label):\n",
    "        label_path = Path(label)\n",
    "        if not label_path.exists():\n",
    "            logging.warning(f\"Label folder not found: {label_path}\")\n",
    "            return\n",
    "\n",
    "        for session_name in os.listdir(label_path):\n",
    "            session_path = os.path.join(label_path, session_name)\n",
    "            if not os.path.isdir(session_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                process_session(session_path, label, output_dir, frame_rate)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing session '{session_path}': {e}\")\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_label, target_extraction_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>sensor_idx</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frame_0000.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.384450</td>\n",
       "      <td>0.453337</td>\n",
       "      <td>0.583412</td>\n",
       "      <td>-0.05100</td>\n",
       "      <td>7.07805</td>\n",
       "      <td>-7.16805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frame_0001.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>-0.007425</td>\n",
       "      <td>-0.029288</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>-0.31200</td>\n",
       "      <td>7.41105</td>\n",
       "      <td>-6.30105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frame_0002.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>198</td>\n",
       "      <td>-0.036162</td>\n",
       "      <td>-0.098587</td>\n",
       "      <td>-0.021588</td>\n",
       "      <td>-0.23700</td>\n",
       "      <td>7.63905</td>\n",
       "      <td>-5.96805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frame_0003.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>297</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>-0.113437</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.54900</td>\n",
       "      <td>7.70595</td>\n",
       "      <td>-5.63295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frame_0004.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>396</td>\n",
       "      <td>-0.060638</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>-0.49095</td>\n",
       "      <td>7.80195</td>\n",
       "      <td>-5.57295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame_file  frame_idx  sensor_idx    gyro_x    gyro_y    gyro_z  \\\n",
       "0  frame_0000.jpg          0           0 -0.384450  0.453337  0.583412   \n",
       "1  frame_0001.jpg          1          99 -0.007425 -0.029288  0.002887   \n",
       "2  frame_0002.jpg          2         198 -0.036162 -0.098587 -0.021588   \n",
       "3  frame_0003.jpg          3         297  0.049087 -0.113437 -0.003437   \n",
       "4  frame_0004.jpg          4         396 -0.060638  0.000550  0.007150   \n",
       "\n",
       "   accel_x  accel_y  accel_z  \n",
       "0 -0.05100  7.07805 -7.16805  \n",
       "1 -0.31200  7.41105 -6.30105  \n",
       "2 -0.23700  7.63905 -5.96805  \n",
       "3 -0.54900  7.70595 -5.63295  \n",
       "4 -0.49095  7.80195 -5.57295  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mapped_df = pd.read_csv(\"extracted_frames/workspace/good_pose/1/frame_sensor_mapping.csv\")\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "DATA_DIR = \"extracted_frames/workspace\"\n",
    "DATA_COLS = [\"gyro_x\", \"gyro_y\", \"gyro_z\", \"accel_x\", \"accel_y\", \"accel_z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "def augment_image(img):\n",
    "    \"\"\"\n",
    "    Apply data augmentation to a single image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Input image as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Augmented image as a NumPy array.\n",
    "    \"\"\"\n",
    "    img_tensor = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "    img_tensor = tf.image.random_flip_left_right(img_tensor)  # Random horizontal flip\n",
    "    img_tensor = tf.image.random_brightness(\n",
    "        img_tensor, max_delta=0.3\n",
    "    )  # Random brightness\n",
    "    img_tensor = tf.image.random_contrast(\n",
    "        img_tensor, lower=0.8, upper=1.2\n",
    "    )  # Random contrast\n",
    "    img_tensor = tf.image.resize_with_crop_or_pad(\n",
    "        img_tensor, IMAGE_SIZE[0] + 10, IMAGE_SIZE[1] + 10\n",
    "    )  # Add padding\n",
    "    img_tensor = tf.image.random_crop(\n",
    "        img_tensor, size=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "    )  # Random crop\n",
    "    img_tensor = tf.image.random_saturation(\n",
    "        img_tensor, lower=0.5, upper=1.5\n",
    "    )  # playing with the saturation\n",
    "\n",
    "\n",
    "    img_tensor = tf.clip_by_value(img_tensor, 0.0, 255.0)  # Clip values\n",
    "    return img_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0, 0, 1], [1, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, min_frames_per_session=1, augmentations_per_image=3):\n",
    "    \"\"\"\n",
    "    Loads images, sensor data, and labels from the dataset directory, with data augmentation.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the dataset directory.\n",
    "        min_frames_per_session (int): Minimum number of frames required to process a session.\n",
    "        augmentations_per_image (int): Number of augmented images to generate per original image.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (images, sensors, labels)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    sensors = []\n",
    "    labels = []\n",
    "    \n",
    "    # \"cant_determien\",\n",
    "    for label_folder in [\"good_pose\", \"bad_pose\"]:\n",
    "        label_path = os.path.join(data_dir, label_folder)\n",
    "        if not os.path.exists(label_path):\n",
    "            logging.warning(f\"Label folder not found: {label_path}\")\n",
    "            continue\n",
    "\n",
    "        # Loop through each session folder\n",
    "        session_folders = [f for f in os.listdir(label_path) if os.path.isdir(os.path.join(label_path, f))]\n",
    "        for session in session_folders:\n",
    "            session_path = os.path.join(label_path, session)\n",
    "\n",
    "            # Find the sensor mapping CSV\n",
    "            csv_file = os.path.join(session_path, \"frame_sensor_mapping.csv\")\n",
    "            if not os.path.exists(csv_file):\n",
    "                logging.warning(f\"Sensor mapping CSV not found: {csv_file}\")\n",
    "                continue\n",
    "\n",
    "            # Load the sensor data from CSV\n",
    "            df = pd.read_csv(csv_file)\n",
    "            session_images = []\n",
    "            session_sensors = []\n",
    "            session_labels = []\n",
    "\n",
    "            # Get all available frame filenames in the session directory\n",
    "            available_frames = {os.path.basename(f) for f in os.listdir(session_path) if f.endswith('.jpg')}\n",
    "            logging.debug(f\"Available frames in {session_path}: {available_frames}\")\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                frame_file_name = row[\"frame_file\"]\n",
    "                if frame_file_name not in available_frames:\n",
    "                    logging.warning(f\"Frame file not found: {frame_file_name}\")\n",
    "                    continue\n",
    "\n",
    "                frame_file_path = os.path.join(session_path, frame_file_name)\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    img = preprocessing.image.load_img(frame_file_path, target_size=IMAGE_SIZE)\n",
    "                    img = preprocessing.image.img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "                    # Apply augmentations\n",
    "                    # augmented_imgs = [img] + [augment_image(img) for _ in range(augmentations_per_image)]\n",
    "                    augmented_imgs = [img]\n",
    "                    # Load sensor data\n",
    "                    sensor_features = np.array([\n",
    "                        row[\"gyro_x\"], row[\"gyro_y\"], row[\"gyro_z\"],\n",
    "                        row[\"accel_x\"], row[\"accel_y\"], row[\"accel_z\"]\n",
    "                    ])\n",
    "\n",
    "                    # Append data\n",
    "                    for augmented_img in augmented_imgs:\n",
    "                        session_images.append(augmented_img)\n",
    "                        session_sensors.append(sensor_features)\n",
    "                        session_labels.append(1 if label_folder == \"good_pose\" else 0)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing frame {frame_file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Check if session has enough frames\n",
    "            if len(session_images) < min_frames_per_session:\n",
    "                logging.warning(f\"Session skipped due to insufficient frames: {session_path}\")\n",
    "                continue\n",
    "\n",
    "            # Add session data to global lists\n",
    "            images.extend(session_images)\n",
    "            sensors.extend(session_sensors)\n",
    "            labels.extend(session_labels)\n",
    "\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    sensors = np.array(sensors, dtype=np.float32)\n",
    "    labels = utils.to_categorical(np.array(labels, dtype=np.int32), num_classes=2)\n",
    "\n",
    "    logging.info(f\"Loaded {len(labels)} samples (including augmented) from {data_dir}.\")\n",
    "    return images, sensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocessing.image.load_img('extracted_frames/workspace/bad_pose/10/frame_0000.jpg', target_size=IMAGE_SIZE)\n",
    "# print(img)\n",
    "img_array = preprocessing.image.img_to_array(img)\n",
    "# print(img_array)\n",
    "augmented_imgs = [img_array] + [augment_image(img) for _ in range(10)]\n",
    "\n",
    "# store teh augmented images in a directory\n",
    "augmented_dir = \"augmented_images\"\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "for i, img in enumerate(augmented_imgs):\n",
    "    img_path = os.path.join(augmented_dir, f\"augmented_{i}.jpg\")\n",
    "    img_pil = preprocessing.image.array_to_img(img)\n",
    "    img_pil.save(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, sensors, labels, validation_split=0.2, test_split=0.1):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, testing, and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        images (np.ndarray): Array of image data.\n",
    "        sensors (np.ndarray): Array of sensor data.\n",
    "        labels (np.ndarray): Array of labels.\n",
    "        validation_split (float): Proportion of the data to use for validation.\n",
    "        test_split (float): Proportion of the data to use for testing.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: (X_img_train, X_img_val, X_img_test, \n",
    "                X_sensor_train, X_sensor_val, X_sensor_test, \n",
    "                y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    dataset_size = len(images)\n",
    "    test_size = int(dataset_size * test_split)\n",
    "    val_size = int(dataset_size * validation_split)\n",
    "    train_size = dataset_size - test_size - val_size\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Split indices\n",
    "    test_indices = indices[:test_size]\n",
    "    val_indices = indices[test_size:test_size + val_size]\n",
    "    train_indices = indices[test_size + val_size:]\n",
    "\n",
    "    # Slice data\n",
    "    X_img_train = images[train_indices]\n",
    "    X_img_val = images[val_indices]\n",
    "    X_img_test = images[test_indices]\n",
    "    X_sensor_train = sensors[train_indices]\n",
    "    X_sensor_val = sensors[val_indices]\n",
    "    X_sensor_test = sensors[test_indices]\n",
    "    y_train = labels[train_indices]\n",
    "    y_val = labels[val_indices]\n",
    "    y_test = labels[test_indices]\n",
    "\n",
    "    return (X_img_train, X_img_val, X_img_test, \n",
    "            X_sensor_train, X_sensor_val, X_sensor_test, \n",
    "            y_train, y_val, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sensor mapping CSV not found: extracted_frames/workspace\\good_pose\\11\\frame_sensor_mapping.csv\n",
      "WARNING:root:Sensor mapping CSV not found: extracted_frames/workspace\\bad_pose\\10\\frame_sensor_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "images, sensors, labels = load_data(DATA_DIR)\n",
    "# Split data using custom splitting logic\n",
    "\n",
    "print(\n",
    "    f\"Loaded data: Images - {images.shape}, Sensors - {sensors.shape}, Labels - {labels.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image training data shape: (11142, 224, 224, 3)\n",
      "Sensor training data shape: (11142, 6)\n",
      "Label training data shape: (11142, 2)\n",
      "Image validation data shape: (3183, 224, 224, 3)\n",
      "Sensor validation data shape: (3183, 6)\n",
      "Label validation data shape: (3183, 2)\n",
      "Image testing data shape: (1591, 224, 224, 3)\n",
      "Sensor testing data shape: (1591, 6)\n",
      "Label testing data shape: (1591, 2)\n"
     ]
    }
   ],
   "source": [
    "X_img_train, X_img_val, X_img_test, X_sensor_train, X_sensor_val, X_sensor_test, y_train, y_val, y_test = split_data(\n",
    "    images, sensors, labels, validation_split=0.2, test_split=0.1\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "print(\"Image training data shape:\", X_img_train.shape)\n",
    "print(\"Sensor training data shape:\", X_sensor_train.shape)\n",
    "print(\"Label training data shape:\", y_train.shape)\n",
    "print(\"Image validation data shape:\", X_img_val.shape)\n",
    "print(\"Sensor validation data shape:\", X_sensor_val.shape)\n",
    "print(\"Label validation data shape:\", y_val.shape)\n",
    "print(\"Image testing data shape:\", X_img_test.shape)\n",
    "print(\"Sensor testing data shape:\", X_sensor_test.shape)\n",
    "print(\"Label testing data shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_img_train\n",
    "# X_sensor_train\n",
    "# y_train\n",
    "\n",
    "# store the data of the training array in a file for showing the data\n",
    "np.save(\"X_img_train.npy\", \n",
    "        # small sample of the data\n",
    "        X_img_train[:10]\n",
    "        )\n",
    "np.save(\"X_sensor_train.npy\", X_sensor_train[:10])\n",
    "np.save(\"y_train.npy\", y_train[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obada\\AppData\\Local\\Temp\\ipykernel_13032\\2874080781.py:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = applications.MobileNetV2(weights='imagenet', include_top=False, input_tensor=image_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "The current used GPU is:  \n"
     ]
    }
   ],
   "source": [
    "# Image input\n",
    "image_input = layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "base_model = applications.MobileNetV2(\n",
    "    weights=\"imagenet\", include_top=False, input_tensor=image_input\n",
    ")\n",
    "\n",
    "image_features = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "# Sensor input\n",
    "sensor_input = layers.Input(shape=(6,))  # 3 gyroscope + 3 accelerometer features\n",
    "sensor_features = layers.Dense(\n",
    "    64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    ")(sensor_input)\n",
    "\n",
    "# Combine features\n",
    "combined = layers.concatenate([image_features, sensor_features])\n",
    "combined = layers.Dropout(0.5)(combined)  # Add dropout before the dense layers\n",
    "combined = layers.Dense(128, activation=\"relu\")(combined)\n",
    "\n",
    "output = layers.Dense(2, activation=\"softmax\")(combined)\n",
    "\n",
    "print(\n",
    "    \"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\"))\n",
    ")  # check if GPU is available\n",
    "# check which gpu is running\n",
    "print(\"The current used GPU is: \", tf.test.gpu_device_name())\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "model = models.Model(inputs=[image_input, sensor_input], outputs=output)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fold cross validation\n",
    "k = 5\n",
    "num_val_samples = len(X_img_train) // k\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1597s\u001b[0m 4s/step - accuracy: 0.9552 - loss: 0.1146 - val_accuracy: 0.7301 - val_loss: 3.2916\n",
      "Epoch 2/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 2s/step - accuracy: 0.9918 - loss: 0.0265 - val_accuracy: 0.4141 - val_loss: 13.3735\n",
      "Epoch 3/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m755s\u001b[0m 2s/step - accuracy: 0.9929 - loss: 0.0224 - val_accuracy: 0.7289 - val_loss: 3.3014\n",
      "Epoch 4/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 2s/step - accuracy: 0.9990 - loss: 0.0047 - val_accuracy: 0.8319 - val_loss: 0.7892\n",
      "Epoch 5/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 2s/step - accuracy: 0.9962 - loss: 0.0139 - val_accuracy: 0.9042 - val_loss: 0.9630\n",
      "Epoch 6/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 2s/step - accuracy: 0.9999 - loss: 5.0656e-04 - val_accuracy: 0.8778 - val_loss: 1.3075\n",
      "Epoch 7/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m770s\u001b[0m 2s/step - accuracy: 0.9958 - loss: 0.0120 - val_accuracy: 0.9259 - val_loss: 0.6358\n",
      "Epoch 8/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 2s/step - accuracy: 0.9987 - loss: 0.0052 - val_accuracy: 0.8373 - val_loss: 1.2554\n",
      "Epoch 9/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 2s/step - accuracy: 0.9915 - loss: 0.0328 - val_accuracy: 0.8065 - val_loss: 1.3683\n",
      "Epoch 10/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m703s\u001b[0m 2s/step - accuracy: 0.9978 - loss: 0.0082 - val_accuracy: 0.8266 - val_loss: 1.2707\n",
      "Epoch 11/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 2s/step - accuracy: 0.9969 - loss: 0.0126 - val_accuracy: 0.6730 - val_loss: 4.3390\n",
      "Epoch 12/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 2s/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 0.8429 - val_loss: 1.0473\n",
      "Epoch 13/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 2s/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 0.9365 - val_loss: 0.3173\n",
      "Epoch 14/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 2s/step - accuracy: 0.9992 - loss: 0.0049 - val_accuracy: 0.8787 - val_loss: 1.2700\n",
      "Epoch 15/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 2s/step - accuracy: 0.9973 - loss: 0.0085 - val_accuracy: 0.8080 - val_loss: 2.2816\n",
      "Epoch 16/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 2s/step - accuracy: 0.9984 - loss: 0.0061 - val_accuracy: 0.9127 - val_loss: 0.9816\n",
      "Epoch 17/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 2s/step - accuracy: 0.9959 - loss: 0.0150 - val_accuracy: 0.7019 - val_loss: 2.8427\n",
      "Epoch 18/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 2s/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 0.8828 - val_loss: 0.7295\n",
      "Epoch 19/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 2s/step - accuracy: 0.9977 - loss: 0.0055 - val_accuracy: 0.9522 - val_loss: 0.5612\n",
      "Epoch 20/20\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.6882e-04 - val_accuracy: 0.9965 - val_loss: 0.0332\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1.0, 1: 2.0}\n",
    "\n",
    "# x trainng \n",
    "# y prediction\n",
    "\n",
    "history = model.fit(\n",
    "    [X_img_train, X_sensor_train],\n",
    "    y_train,\n",
    "    validation_data=([X_img_val, X_sensor_val], y_val),\n",
    "    class_weight=class_weight,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 389ms/step - accuracy: 0.6370 - loss: 0.6378\n",
      "Validation Accuracy: 63.86%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [X_img_val, X_sensor_val],\n",
    "    y_val,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Validation Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# load the model \n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "model = models.load_model(\"multi_modal_posture_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([X_img_test, X_sensor_test], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m      4\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([X_img_test, X_sensor_test], batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R): 0.0953\n",
      "Mean Squared Error (MSE): 0.2262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Calculate R-squared and Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R-squared (R): {r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6346042156219482, 0.6385920643806458)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 17:26:38,041 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-01-09 17:26:39,850 - INFO - Function `function` contains input name(s) resource with unsupported characters which will be renamed to functional_12_1_dense_38_1_biasadd_readvariableop_resource in the SavedModel.\n",
      "2025-01-09 17:26:40,091 - INFO - Function `function` contains input name(s) resource with unsupported characters which will be renamed to functional_12_1_dense_38_1_biasadd_readvariableop_resource in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\obada\\AppData\\Local\\Temp\\tmp4c62n6_9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 17:26:45,563 - INFO - Assets written to: C:\\Users\\obada\\AppData\\Local\\Temp\\tmp4c62n6_9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\obada\\AppData\\Local\\Temp\\tmp4c62n6_9'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_1920'), TensorSpec(shape=(None, 6), dtype=tf.float32, name='keras_tensor_2075')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2247513000592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247513008512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247513010448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247513004992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247504420304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247512999536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247512999712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247513006224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247512998128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242515327584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247512999360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721798048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721805088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721796112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721792944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242515323536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721801040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721793824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721792416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721797344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721803680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247133225280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247133230032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247133232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247133227392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2242721804032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519780928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519781104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247133232320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519782688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247133225104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519787264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519789904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519785856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519787792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519796240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519794480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519791840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519786208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519795536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519792192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519867776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519870416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519866368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519868304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519793424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519875344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519877984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519873936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519875872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519874288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519966080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519968896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519964672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519967136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519974704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519963264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519974880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519972240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519974000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519976816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519618496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519617616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519620784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519620256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519627648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519625888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519628704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519617440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519626944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519631344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520075840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520076016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519632400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520077600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247519624832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520082880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520086048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520082176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520085168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520088512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520087632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520087984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520089568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520091856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520088336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520114768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520117408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520113360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520115296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520090624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520122336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520119168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520120928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520122864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520121280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520180304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520183120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520178896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520181360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520188928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520187872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520184704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520178016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520188400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520179248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520308912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520311552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520307504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520309440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520317888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520316128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520318944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520307856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520317184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520313840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520405632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520408448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520403872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520407392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520315072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520413376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520416016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520411968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520413904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520418480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520471168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520469408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520417776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520471520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520418304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520477504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520480144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520476096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520478032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520482960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520551680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520552736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520484016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520550976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520482256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520558016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520560832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520556608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520559072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520566640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520563120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520562416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520563824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520562768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520556960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520638880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520641520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520637472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520639408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520647856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520646096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520648912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520637824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520647152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520643808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520752160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520754800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520750752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520752688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520645040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520758320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520760960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520756912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520758848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520763424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520848880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520848352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520849408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520849232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520763248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520855216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520857856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520853808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520855744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520860672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520944720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520944368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520859968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520946480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520953872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520952112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520954928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520947008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520953168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520957568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520957216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520957392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520958624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521027520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247520951056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521032976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521035616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521031568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521033504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521039488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521124944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521038256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521041952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521125472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521039312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521131280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521133920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521129872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521131808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521041072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521137440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521140080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521136032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521137968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521136384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521210384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521213200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521208976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521211440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521219008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521217952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521220592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521208096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521218480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521209328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521307808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521310272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521305696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521309216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521316608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521314848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521317664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521305872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521315904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521320304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521371584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521370704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521373872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521373344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521313792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521379328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521381968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521377920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521379856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521384432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521469888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521468656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521386016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521470240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521384256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521476224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521478864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521474816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521476752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521483616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521480096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521479392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521481328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521483440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521482912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521593552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521597600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521594432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521596016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2247521596368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"multi_modal_posture_model.h5\")\n",
    "# save as tensorflow light model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open(\"multi_modal_posture_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impored_model = models.load_model(\"multi_modal_posture_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obada\\AppData\\Local\\Programs\\Python\\Python313\\Scripts\\jupyter-nbconvert.EXE\\__main__.py:4: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "[NbConvertApp] Converting notebook C:\\Users\\obada\\Desktop\\HoldWise Dataset\\posture_classifier.ipynb to script\n",
      "[NbConvertApp] Writing 19544 bytes to C:\\Users\\obada\\Desktop\\HoldWise Dataset\\posture_classifier.py\n",
      "C:\\Users\\obada\\AppData\\Local\\Programs\\Python\\Python313\\Scripts\\jupyter-nbconvert.EXE\\__main__.py:4: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "[NbConvertApp] Converting notebook C:\\Users\\obada\\Desktop\\HoldWise Dataset\\posture_classifier.ipynb to html\n",
      "[NbConvertApp] Writing 413757 bytes to C:\\Users\\obada\\Desktop\\HoldWise Dataset\\posture_classifier.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script \"C:\\Users\\obada\\Desktop\\HoldWise Dataset\\posture_classifier.ipynb\"\n",
    "!jupyter nbconvert --to html \"C:\\Users\\obada\\Desktop\\HoldWise Dataset\\posture_classifier.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
